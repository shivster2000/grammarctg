{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15365f89-66d7-45e1-ad97-dc4610aeeeb5",
   "metadata": {},
   "source": [
    "# Exp018: Instruction fine-tuning for constrained text generation\n",
    "This experiment aims at instruction fine-tuning from existing skills in the dataset to train the model on single constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c19775e-4db4-4d58-9310-0e82d15f32ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 19:13:14.690154: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /cluster/home/dglandorf/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "os.environ['CACHE_DIR'] = f\"/scratch/tmp.{os.getenv('SLURM_JOB_ID')}.dglandorf\" # speed up model loading\n",
    "os.environ['WANDB_DIR'] = os.getenv('CACHE_DIR')\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "import pickle\n",
    "from torch.utils.data import RandomSampler\n",
    "import numpy as np\n",
    "import json\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "import sys\n",
    "sys.path.append(f'../source')\n",
    "import helpers\n",
    "import models\n",
    "import evaluation\n",
    "import importlib\n",
    "#importlib.reload(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d860d9f7-cfd7-4c60-8ab7-ac59fa12d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "input_file = '../data/corpus_classification_all.pkl'\n",
    "preprossed_dataset_file = '../data/SFT_data.jsonl'\n",
    "checkpoint_dir = '/cluster/scratch/dglandorf/models/'\n",
    "nrs = list(evaluation.detector.classifiers.keys())\n",
    "nrs_to_consider = [1175]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1bf8eb-e0b0-4d72-92de-39d59362eafa",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f860be6b-ba7d-498f-9147-f65d5d8b1ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(preprossed_dataset_file):\n",
    "    with open(input_file, 'rb') as f:\n",
    "        all_hit_indices = pickle.load(f)\n",
    "        all_hit_sentences = pickle.load(f)\n",
    "        extracts = pickle.load(f)\n",
    "    \n",
    "    data = [{\"context\": extracts[idx][0],\n",
    "             \"response\": extracts[idx][1],\n",
    "             \"constraints\": [nr],\n",
    "             \"source\": extracts[idx][2],} for nr in nrs for idx in all_hit_indices[nr]]\n",
    "    \n",
    "    with open(preprossed_dataset_file, 'w') as f:\n",
    "        for item in tqdm(data):\n",
    "            f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e2f6bc-a141-4832-8f94-af0732f0b8da",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb69f077-228e-4054-95a7-9bd8c359bd4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef43905a5ac34235bf1854dca033dbc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('json', data_files=preprossed_dataset_file, split='train', cache_dir=os.getenv('CACHE_DIR'))\n",
    "dataset = dataset.filter(lambda item: any(item['constraints']==[nr] for nr in nrs_to_consider))\n",
    "dataset = dataset.map(helpers.get_generation_prompt)\n",
    "train_test_split = dataset.train_test_split(test_size=100)\n",
    "train_dataset, test_dataset = train_test_split['train'], train_test_split['test']\n",
    "\n",
    "unconstrained = test_dataset.map(helpers.get_generation_prompt, fn_kwargs={\"unconstrained\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e897370-34b9-4776-9d6c-5eaa66f24db6",
   "metadata": {},
   "source": [
    "## Load and prepare base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b489244c-bcab-4da7-ad02-abbce1048030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500910710a0a4348a5316e7b00257df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = models.load_generator()\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2302a546-0eff-452a-b195-dbe6ceede8a9",
   "metadata": {},
   "source": [
    "### Inference with current model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a331450b-9ac4-4a40-9e2a-2758706a076e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Write the response of A and include these grammatical items in the response:\n",
      "- negation: Can form negative statements of main verb 'be', with contracted and uncontracted forms.\n",
      "Dialog:\n",
      "A: Steven Curry was a great basketball player.\n",
      "B: i think he still is.  his 3 point shooting is his signature to the Warriors success.  \n",
      "A: That blowout against the rockets was amazing.\n",
      "B: yes, the Warriors go as Curry goes.  Steph feeds off the energy from the crowd as Steve Kerr has pointed out. [/INST] \n",
      "A: Its a shame he wasn't paid more when part of the warriors since he seemed to make the team.</s>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef9674f57b6401db620ff4cbbe697c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Steven Curry was a great basketball player. (Positive statement)\\nB: I don't agree. While he was great in the past, his skills have declined. (Negation of A's statement)\\n\\nA: That blowout against the Rockets was amazing. (Positive statement)\\nB: I saw the game too. But the Warriors haven't been that dominant lately. (Negation of the implied statement in A's statement)\\n\\nA: Can form negative statements of main verb 'be', with contracted and uncontracted forms.\\nB: Yes, you'\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Steven Curry was a great basketball player. (Positive statement)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_dataset['text'][7])\n",
    "models.generate(model, tokenizer, [test_dataset['prompt'][7]], skip_special_tokens=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd49778e-65db-444d-8833-022fefad35a8",
   "metadata": {},
   "source": [
    "## Evaluate outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1f788de-4d48-497b-99fd-add052918ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds, verbose=False, n=25, datasets={\"train\": train_dataset, \"test\": test_dataset}, eval_quality=False, ground_truth=False):\n",
    "    results = {}\n",
    "    for name, ds in datasets.items():\n",
    "        subset = dataset[RandomSampler(ds, num_samples=n)]\n",
    "        if verbose: print(subset['prompt'][0])\n",
    "        if ground_truth:\n",
    "            outputs = subset['response']\n",
    "        else:\n",
    "            outputs = models.generate(model, tokenizer, subset['prompt'])\n",
    "        scores, distinct, quality = evaluation.calc_metrics(subset['context'], outputs, subset['constraints'], eval_quality)\n",
    "        if verbose:\n",
    "            for truth, output in zip(subset['response'], outputs):\n",
    "                print(f\"Truth: {truth}\")\n",
    "                print(f\"Gener: {output}\")\n",
    "            print(f\"Grammar detected: {scores}\")\n",
    "            print(f\"Distinctiveness per constraint {distinct}\")\n",
    "            print(f\"Quality: {quality}\")\n",
    "        print(list(zip(outputs,scores))[:10])\n",
    "        \n",
    "        results.update({f\"{name}_constraint\": np.mean(scores)})\n",
    "        results.update({f\"{name}_{metric}\": np.mean(quality[metric]) for metric in quality.keys()})\n",
    "        results.update({f\"{name}_distinct\": np.mean(distinct)})        \n",
    "    return results\n",
    "\n",
    "#compute_metrics([], verbose=False, n=25, datasets={\"test\": test_dataset}, eval_quality=False, ground_truth=False) # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51e5c22b-ff9d-4bad-8d62-850e6b95e332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"Maybe.  Or the voice actor wasn't available for that episode?  I didn't know that there were other people who did the voice for Thomas the Tank.  In addition to George Carlin, Ringo Starr and Alec Baldwin did it as well.   In fact, all 3 did the voice for at  least 52 episodes each.\", 1.0), (\"I got my kids a spiderman costume! Isn't that wonderful?\", 1.0), (\"The gallery's on Flower Street, isn't it?\", 1.0), ('That\\'s amazing isn\\'t it! Also she won three Primetime Emmy Awards oh, she just can\\'t stop winning! And yes, Dino De Laurentiis said to his son in Italian \"she\\'s ugly why did you bring me this thing\" but he didn\\'t know that she spoke Italian and she then replied to him LOL', 1.0), (\"I expect people to resist technology, especially when human beings are not there as a safety measure. I can't understand how it's illegal to warm up your car in Ohio. Isn't it cold there?\", 1.0), ('I am not real sure, but she did get alot of praise on her performance. ', 1.0), ('yeah today Education frequently takes place under the guidance of educators but some are not qualified', 1.0), (\"I wish it wasn't, but there is actually a lot of crime and prostitution around here. \", 1.0), (\"He came from a broken home, and his mother struggled to work long hours. He says his childhood was rough, so it's curious to me how he's so strong and isn't overly stressed by the types of things he goes through with many of his cases.\", 1.0), (\"I was unaware of the iphone fact. Being on a farm do you still have one of the old style radios? I think it would be pretty fun if they still broadcasted radio acts on the air like they used to in the 1930's. They have a radio station that translates solar activity into sound, but not sure how interesting that would be.\", 1.0)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2511e86076d840529a7253ba934a1e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"While it's true that red hair is less common than other hair colors, it's not accurate to say that it's a dying breed. In fact, around 2-6% of people with Western European ancestry have red hair.\", 0.0), ('Mick Jagger is the lead vocalist. He is quite popular. ', 0.0), (\"I don't have an iPhone with that level of processing power. I'm not against technology, but I don't feel the need to upgrade my phone that frequently. As for Aunt's homophobic views, I can't change her mind, and I don't think it's productive to argue with her about it. It's unfortunate, but we all have to deal with difficult family members.\", 0.0), (\"I'm not finished with our conversation yet. Your button isn't working? \", 1.0), (\"I'm sorry, I misunderstood. You didn't say Town Hall, but rather the East Town Mall?\", 0.0), (\"Well said, B. However, I must disagree with your assessment. Contrary to your belief, it's not necessarily true that the loser and the winner are not on the same page. Negating your statement, they could potentially share similar goals or values. \", 1.0), (\"The guy who discovered classic cars in his barn on his new property must have been pleased! I'm curious, though, what he ended up doing with them.\", 0.0), (\"While they have indeed achieved impressive feats with millions of records sold and sixteen Grammy awards, it's important to note that no other musical act can be directly compared to Red Hot Chili Peppers due to their unique sound and style. \", 0.0), (\"The guy who discovered classic cars in his barn on his new property must have been pleased! I'm curious, though, what he ended up doing with them.\", 0.0), (\"That's correct, it was released in 2013.\", 0.0)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10aae7bd9a7d46629060bf2fbca56a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"During his highschool days, he didn't need to be self-sufficient in sports training. His mother didn't just help him, she coached him extensively. He didn't only make acquaintances, he formed some genuine emotional bonds.\", 0.0), ('Some lizards can jump from high places, but not all of them do. As for their size, it varies greatly among the different species. For instance, the Komodo dragon is one of the largest living lizards, growing up to 10 feet long, while others, like geckos, are quite small.', 0.0), ('Basketball isn\\'t too good, I mean it isn\\'t too bad... Do you know they have a basketball court in the Supreme Court building? They call it \"the highest court in the land.\" Wonder who they get to play there?', 1.0), (\"I see. Well, Cathy isn't here right now. She's not in the dining room. \", 1.0), (\"No, not even with powerful machines like monster trucks from the monster truck rally. That's just an urban legend. The pages of two phonebooks can actually be separated.\", 0.0), (\"Yes, there are classes available for learning those dance moves. However, it's important to note that not all of them may be offered in every location. For instance, bomba and plena, which originated in Puerto Rico, might be more difficult to find outside of Puerto Rico or other Caribbean islands. But with the popularity of salsa, many dance studios and schools offer classes in salsa, cha-cha-cha, and mambo. If you're interested, I would recommend checking with local dance studios or dance organizations to see what they offer. Alternatively, you could look for workshops\", 0.0), (\"I'm not suggesting that there isn't a bed and breakfast in Minnesota with such an offer. However, I was just making a hypothetical statement about the possibility of finding one.\", 1.0), (\"Yes, he's had some surgeries, but not all of them. He's still in the process and making decisions about what he wants to do. \", 0.0), ('No, the palaÄinke originated in Central Europe, specifically in the Austro-Hungarian Empire. Despite not being the birthplace, I still find their thin, crepe-like texture quite appealing.', 0.0), (\"I see. But Spiderman doesn't have that backstory when he's in his human form, doesn't he?\", 0.0)]\n"
     ]
    }
   ],
   "source": [
    "all_metrics = {}\n",
    "all_metrics.update(compute_metrics([], n=25, datasets={\"truth\": unconstrained}, eval_quality=False, ground_truth=True))\n",
    "all_metrics.update(compute_metrics([], n=25, datasets={\"base\": test_dataset}))\n",
    "all_metrics.update(compute_metrics([], n=25, datasets={\"unconstrained\": unconstrained}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90b8ed6-4a6a-4237-acc5-e0633d076a4b",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37309c21-7fb3-487c-bc04-5f1d4f7e16e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e6871a9cfb46e786fd2451a3d44744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32001, 4096)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, tokenizer = models.load_generator(quantized=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = '[PAD]'\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "125626b6-a4ab-41eb-8940-fa8fa0745d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7f12f77-b126-4955-9f1d-b780464ff7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'{checkpoint_dir}mistral_FT_{\"_\".join(str(nr) for nr in nrs_to_consider)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cbf1b82-b32e-4c18-9905-35375303cf0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/cluster/scratch/dglandorf/models/mistral_FT_1175'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b8ee436-02f3-47b9-b605-830bff043f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    #num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=5,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=500,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"gctg\",\n",
    "    load_best_model_at_end=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    per_device_eval_batch_size=4,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    save_total_limit=1,\n",
    "    save_only_model=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa48307e-a483-404f-bb12-170ef022ff60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b530926dd5f4209850a41c6e5309d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21874 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da90f5d4b30d4311815dea6783643e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/dglandorf/gctg/lib64/python3.11/site-packages/trl/trainer/sft_trainer.py:317: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/cluster/home/dglandorf/gctg/lib64/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    "    data_collator=DataCollatorForCompletionOnlyLM(\"[/INST]\", tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    "    #neftune_noise_alpha=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "255a8ea4-8027-4101-8a7e-fbfa620066d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdo-gl\u001b[0m (\u001b[33mdomgla\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/tmp.54483290.dglandorf/wandb/run-20240410_190458-19f60zim</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/domgla/huggingface/runs/19f60zim' target=\"_blank\">gctg</a></strong> to <a href='https://wandb.ai/domgla/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/domgla/huggingface' target=\"_blank\">https://wandb.ai/domgla/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/domgla/huggingface/runs/19f60zim' target=\"_blank\">https://wandb.ai/domgla/huggingface/runs/19f60zim</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 55/500 03:23 < 28:28, 0.26 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Train Constraint</th>\n",
       "      <th>Train Distinct</th>\n",
       "      <th>Test Constraint</th>\n",
       "      <th>Test Distinct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.015000</td>\n",
       "      <td>1.946367</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.297663</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.196907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201aca56172f47449e29f04356695897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"I'm not sure,  I'm not sure I'd want my dog to be the one to donate blood.\", 1.0), (\"I don't think she is too happy. I think she is just happy to be a police officer.\", 1.0), (\"I'm not sure I'm that big of a fan of Pokemon, but I do like the show. I'm not sure I'm that big of a fan of Pokemon, but I do like the show.\", 1.0), (\"Not necessarily. I'm not interested in a girl's money. \", 1.0), (\"I'm not sure.  I'm not sure if he was the MVP in 2017.  I'm not sure if he was the MVP in 2018.  I'm not sure if he was the MVP in 2019.  I'm not sure if he was the MVP in 2020.  I'm not sure if he was the MVP in 2021.  I'm not sure if he was the MVP in 2022.  I'm not sure if he\", 1.0), (\"I'm not sure about that. I'm not sure that we'll win.\", 1.0), (\"I'm not sure about that. I'm not sure if that's true or not. I'm not sure if it's even possible. I'm not sure if it's even possible to put a CD in a PS3. I'm not sure if it's even possible to put a CD in a PS4. I'm not sure if it's even possible to put a CD in a PS5. I'm not sure if it's even possible to put a CD in a PS6. I'm not sure if it's even possible to put a CD\", 1.0), (\"1mb of ram isn't much at all, is it?\", 1.0), (\"I'm not sure I'm ready for that.\", 1.0), (\"I'm not sure, I'm not really into metal, but I'm not sure what you mean by a comeback, I'm not sure what you mean by a comeback, I'm not sure what you mean by a comeback, I'm not sure what you mean by a comeback, I'm not sure what you mean by a comeback, I'm not sure what you mean by a comeback, I'm not sure what you mean by a comeback, I'm not sure what you mean by a comeback, I\", 1.0)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be80a77fb2e14bd0b985ff735d1cd79f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"I'm not sure I'm ready for that. I'm not sure I'm ready for a world where people don't have to drive.\", 1.0), ('Not sure.', 1.0), ('I am not sure.  I am not sure if there is a cure for food allergies.', 1.0), (\"Not sure, I'm not a big fan of animals.  I'm not sure I'd want to sleep with a cat, but I guess it's not for me.\", 1.0), (\"I'm not sure what I'm in the mood for. I'm not sure what I'm in the mood for.\", 1.0), (\"I'm not sure, but I'm not sure that's the case. I'm not sure what it is, but I'm not sure that's the case. I'm not sure what it is, but I'm not sure that's the case. I'm not sure what it is, but I'm not sure that's the case. I'm not sure what it is, but I'm not sure that's the case. I'm not sure what it is, but I'm not sure that's the case. I'm not sure\", 1.0), ('Not sure about that.', 1.0), ('I am not surprised. ', 1.0), ('I am not sure. I am not sure.', 1.0), ('I am not sure.  I am not sure how much he made in his last season.  I am not sure how much he made in his last season.  I am not sure how much he made in his last season.  I am not sure how much he made in his last season.  I am not sure how much he made in his last season.  I am not sure how much he made in his last season.  I am not sure how much he made in his last season.  I am not sure how much he made in his last season.  I am not sure how much he made in his', 1.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/dglandorf/gctg/lib64/python3.11/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gctg/lib64/python3.11/site-packages/trl/trainer/sft_trainer.py:360\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 360\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m~/gctg/lib64/python3.11/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gctg/lib64/python3.11/site-packages/transformers/trainer.py:2118\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2124\u001b[0m ):\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/gctg/lib64/python3.11/site-packages/transformers/trainer.py:3045\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3043\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3044\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3045\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3047\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/gctg/lib64/python3.11/site-packages/accelerate/accelerator.py:2001\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1999\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2000\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2001\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gctg/lib64/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gctg/lib64/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5250aaa3-02de-4794-ba53-19d3e0c233d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4434c38-76a7-4cef-873b-0377c874d4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8214cb7e36948279679af191e68592c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"I'm not sure. I'm not sure who you could compare them to. I'm not sure who is in the same league as them.\", 1.0), ('I am not sure, I am not sure what else there is to do there. I am not sure if there is a lot of nightlife or not. I am not sure if there is a lot of shopping or not. I am not sure if there is a lot of restaurants or not. I am not sure if there is a lot of museums or not. I am not sure if there is a lot of parks or not. I am not sure if there is a lot of historical sites or not. I am not sure if there is a lot of sports or not. I am not sure if there is a lot of', 1.0), (\"I'm not sure that's a good thing.\", 1.0), ('Not sure. ', 1.0), ('Not sure.', 1.0), ('13% of the population of Scotland has red hair', 0.0), (\"I'm not sure. I'm not really sure. I'm not sure if you're familiar with the area. \", 1.0), (\"I didn't know that.  I guess I am not surprised.  I am not sure what I would do without my smartphone.  I am not sure how I would have survived the pandemic without it. \", 1.0), ('I am not surprised. ', 1.0), ('I am not sure, but I am not sure if they were used in foxholes.', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "all_metrics.update(compute_metrics([], verbose=False, datasets={\"test\": test_dataset}, n=100, eval_quality=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ea6b714-3770-43c9-b9d2-118e829696f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'truth_constraint': 1.0,\n",
       " 'truth_distinct': 0.8324175824175825,\n",
       " 'base_constraint': 0.28,\n",
       " 'base_distinct': 0.7886363636363637,\n",
       " 'unconstrained_constraint': 0.2,\n",
       " 'unconstrained_distinct': 0.8415094339622642,\n",
       " 'test_constraint': 0.86,\n",
       " 'test_distinct': 0.2015105740181269}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "636b45ea-992a-473a-be9e-e9a627d03f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{output_dir}/metrics.json\", 'w') as file:\n",
    "    json.dump(all_metrics, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "647989a0-c095-48eb-bb71-52f1b52a091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model, tokenizer = models.load_generator()\n",
    "model = PeftModel.from_pretrained(model, f\"{output_dir}/checkpoint-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c16fb416-b4c7-4719-94c7-e8be23b7f673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_metric': 1.9463669061660767, 'best_model_checkpoint': '/cluster/scratch/dglandorf/models/mistral_FT_1175/checkpoint-50', 'epoch': 0.0091424392027793, 'eval_steps': 50, 'global_step': 50, 'is_hyper_param_search': False, 'is_local_process_zero': True, 'is_world_process_zero': True, 'log_history': [{'epoch': 0.0, 'grad_norm': 7.839364051818848, 'learning_rate': 3.3333333333333335e-05, 'loss': 5.8733, 'step': 5}, {'epoch': 0.0, 'grad_norm': 5.5172295570373535, 'learning_rate': 6.666666666666667e-05, 'loss': 4.5078, 'step': 10}, {'epoch': 0.0, 'grad_norm': 2.364335775375366, 'learning_rate': 0.0001, 'loss': 2.6739, 'step': 15}, {'epoch': 0.0, 'grad_norm': 1.4819278717041016, 'learning_rate': 9.896907216494846e-05, 'loss': 2.2602, 'step': 20}, {'epoch': 0.0, 'grad_norm': 1.9767122268676758, 'learning_rate': 9.793814432989691e-05, 'loss': 2.0064, 'step': 25}, {'epoch': 0.01, 'grad_norm': 1.7913768291473389, 'learning_rate': 9.690721649484537e-05, 'loss': 1.9699, 'step': 30}, {'epoch': 0.01, 'grad_norm': 1.773166537284851, 'learning_rate': 9.587628865979382e-05, 'loss': 1.7949, 'step': 35}, {'epoch': 0.01, 'grad_norm': 2.0969133377075195, 'learning_rate': 9.484536082474227e-05, 'loss': 1.9755, 'step': 40}, {'epoch': 0.01, 'grad_norm': 2.1509501934051514, 'learning_rate': 9.381443298969073e-05, 'loss': 1.9214, 'step': 45}, {'epoch': 0.01, 'grad_norm': 3.781041145324707, 'learning_rate': 9.278350515463918e-05, 'loss': 2.015, 'step': 50}, {'epoch': 0.01, 'eval_loss': 1.9463669061660767, 'eval_runtime': 100.2976, 'eval_samples_per_second': 0.997, 'eval_steps_per_second': 0.249, 'eval_test_constraint': 0.96, 'eval_test_distinct': 0.19690721649484536, 'eval_train_constraint': 0.92, 'eval_train_distinct': 0.2976629766297663, 'step': 50}], 'logging_steps': 5, 'max_steps': 500, 'num_input_tokens_seen': 0, 'num_train_epochs': 1, 'save_steps': 50, 'total_flos': 1696006226672640.0, 'train_batch_size': 4, 'trial_name': None, 'trial_params': None}\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{output_dir}/checkpoint-50/trainer_state.json\", 'r') as file:\n",
    "    loaded_data = json.load(file)\n",
    "    print(loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9434c8-cd2d-4b5c-af41-26e36ec5ded2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gctg)",
   "language": "python",
   "name": "gctg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
