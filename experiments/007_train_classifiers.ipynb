{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50c32296-73fb-4cb5-a6ad-0a26a59be04c",
   "metadata": {},
   "source": [
    "# Exp007: Train grammar rule classifier\n",
    "This experiment elaborates different ways to train a binary classifier to detect the usage of an EGP rule in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87ba234-c82d-4a00-ba5e-1c9e0bb2f146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b5e66d3b-cfe0-4c90-8a64-6cb8568cfb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleDetector(torch.nn.Module):\n",
    "    def __init__(self, bert_encoder, hidden_dim=32, dropout_rate=0.25, train_bert=False):\n",
    "        super().__init__()\n",
    "        self.bert = bert_encoder\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = train_bert\n",
    "        input_dim = self.bert.config.hidden_size*(self.bert.config.num_hidden_layers+1)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.hidden = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.output = torch.nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, diagnose=False):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert(input_ids, attention_mask)\n",
    "            x = torch.cat(outputs.hidden_states, dim=-1)\n",
    "        if diagnose:\n",
    "            print(x.shape)\n",
    "        x = self.dropout(x)\n",
    "        x = self.hidden(x)\n",
    "        if diagnose:\n",
    "            print(x.shape)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        x = self.sigmoid(x)\n",
    "        if diagnose:\n",
    "            print(x)\n",
    "        x = x * attention_mask.unsqueeze(-1)\n",
    "        if diagnose:\n",
    "            print(x)\n",
    "        \n",
    "        max_values, max_indices = torch.max(x, 1)\n",
    "        return max_values.flatten(), max_indices.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0186655-581e-4419-ba2a-9fca2c2761d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_len):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(self.sentences[idx], return_tensors='pt', max_length=self.max_len, padding='max_length')\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "def get_dataset(row, tokenizer, max_len, df, random_negatives=True, ratio = 0.5, max_positive_examples=500):\n",
    "    # assemble dataset for one construction\n",
    "    # 50% positive examples\n",
    "    unique_examples = list(set(row['augmented_examples']))\n",
    "    sentences = unique_examples[:max_positive_examples]\n",
    "    labels = [1] * len(sentences)\n",
    "\n",
    "    num_augs = int(len(sentences) * (1-ratio)) if random_negatives else len(sentences)\n",
    "    # augmented negative examples\n",
    "    aug_neg_examples = list(set(row['augmented_negative_examples']).difference(set(row['augmented_examples'])))\n",
    "    random.shuffle(aug_neg_examples)\n",
    "    unique_negatives = aug_neg_examples[:num_augs]\n",
    "    sentences += unique_negatives\n",
    "    labels += [0] * len(unique_negatives)\n",
    "    \n",
    "    if random_negatives:\n",
    "        num_rands = max_positive_examples - len(unique_negatives) # fill to an even number\n",
    "        # rest: random negative examples (positive from other constructions)\n",
    "        neg_examples = [example for sublist in df.loc[df['#'] != row['#'], 'augmented_examples'].to_list() for example in sublist]\n",
    "        random.shuffle(neg_examples)\n",
    "        sentences += neg_examples[:num_rands]\n",
    "        labels += [0] * len(neg_examples[:num_rands])\n",
    "    assert len(sentences) == 2 * max_positive_examples\n",
    "    assert sum(labels) == max_positive_examples\n",
    "    return SentenceDataset(sentences, labels, tokenizer, max_len)\n",
    "\n",
    "def get_loaders(dataset, batch_size=16):\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(0.8 * total_size)\n",
    "    val_size = total_size - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44da9976-2633-4816-9048-b85fdf4fdb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', cache_dir=os.getenv('CACHE_DIR'))\n",
    "bert_encoder = BertModel.from_pretrained('bert-base-uncased', cache_dir=os.getenv('CACHE_DIR'), output_hidden_states=True)\n",
    "classifier = RuleDetector(bert_encoder).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "843e27fd-c6c0-42d6-b14e-30e382274478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 319553\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in classifier.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1a2f87-7b37-425a-beae-3f34f5be5bdf",
   "metadata": {},
   "source": [
    "Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0196c0e-e5b7-4bd4-ba7f-72b90fb558a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5531], device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"Switzerland is a beautiful country.\"\n",
    "encoded_input = bert_tokenizer(input_text, return_tensors='pt').to(device)\n",
    "max_values, max_indices = classifier(encoded_input['input_ids'], encoded_input['attention_mask'])\n",
    "max_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050fd0d3-3545-4d9d-9811-d25c9d66f315",
   "metadata": {},
   "source": [
    "Train the rule detector for one random rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da9ef98f-1b37-4b61-a606-2e2c15ac69ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "egp_examples = pd.read_json(\"../data/egp_examples.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "ce1c2f26-fadd-4885-94f4-9b57d9d35574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can use 'have (got) to' to talk about obligations.\n",
      "The concert starts at midnight but we have to go before then because we have got to buy our tickets. \n",
      "\n",
      "You have to bring your swimming costume.\n",
      "['They have to complete the online training module by Thursday.', 'She has to take her medicine at the same time every day.', 'We have to leave for the airport at 6 am.', 'We have got to send out the invitations for the party soon.', 'She has got to take her medicine after dinner.', 'We have to wake up early for our flight.', 'We’ve got to buy groceries after work.', 'I have to jog in the mornings to stay fit.', \"We've got to leave early to avoid the traffic.\", 'He has to submit his report by the end of the day.']\n",
      "['She returns the library books before they become overdue.', 'We plan our vacation soon.', 'She practices the piano before her recital.', 'Do you take the dog for a walk every morning?', 'She buys groceries for dinner tonight.', 'I finish my homework before I can go out and play.', 'Tom wants to clean his room this weekend.', 'The children tidied their rooms before they played outside.', 'I want to finish my homework before dinner.', 'He pays his electricity bill before it gets disconnected.']\n"
     ]
    }
   ],
   "source": [
    "rule = egp_examples.sample(1).iloc[0]\n",
    "print(rule['Can-do statement'])\n",
    "print(rule['Example'])\n",
    "print(random.sample(rule['augmented_examples'], 10))\n",
    "print(random.sample(rule['augmented_negative_examples'], 10))\n",
    "classifier = RuleDetector(bert_encoder).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(classifier.parameters(), 1e-4)\n",
    "dataset = get_dataset(rule, bert_tokenizer, 64, egp_examples) \n",
    "train_dataloader, val_dataloader = get_loaders(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "868049f6-9dbd-4bf4-baab-945b5c457f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RuleDetector(bert_encoder).to(device)\n",
    "optimizer = torch.optim.AdamW(classifier.parameters(), 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "90a2f2f7-0ad2-41da-8035-34bf5aa3499f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 36.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.400087109208107\n",
      "Accuracy: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 38.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.16590621560811997\n",
      "Accuracy: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 37.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.10522952415049076\n",
      "Accuracy: 0.975\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_dataloader, val_dataloader, num_epochs=3, criterion = torch.nn.BCELoss()):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_dataloader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            values, _ = model(input_ids, attention_mask=attention_mask, diagnose=False)\n",
    "            loss = criterion(values, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(f'Training loss: {avg_train_loss}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval() \n",
    "        total_correct = 0\n",
    "        total_examples = 0\n",
    "        \n",
    "        with torch.no_grad():  # No gradients needed for validation\n",
    "            for batch in val_dataloader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "    \n",
    "                outputs, _ = model(input_ids, attention_mask)\n",
    "                predictions = outputs > 0.5                \n",
    "                total_correct += (predictions.flatten() == labels).sum().item()\n",
    "                total_examples += labels.size(0)\n",
    "\n",
    "        accuracy = total_correct / total_examples\n",
    "        print(f'Accuracy: {accuracy}')\n",
    "        \n",
    "train(classifier, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "fa8e3598-66a6-46f4-b6c2-581e01a93017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9988191723823547\n",
      "Maximum token: 6\n"
     ]
    }
   ],
   "source": [
    "input_text = \"The class was on Monday. It started at 6:00 pm and finished at 7:00 pm.\"\n",
    "encoded_input = bert_tokenizer(input_text, return_tensors='pt', max_length=64, padding='max_length').to(device)\n",
    "with torch.no_grad():\n",
    "    values, indices = classifier(encoded_input['input_ids'], encoded_input['attention_mask'], diagnose=False)\n",
    "    print(f'Score: {values.item()}')\n",
    "tokens = bert_tokenizer.convert_ids_to_tokens(encoded_input['input_ids'].squeeze().tolist())\n",
    "print(f'Maximum token: {tokens[indices]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4fb678-4cbb-4cab-b2d4-c8d00d92e099",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
